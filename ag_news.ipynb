{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1225b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import csv\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e7d44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7789ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ctm_nlp import CTM_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff797a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE_LIMIT = 10000\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "CTM_D_MODEL = 256\n",
    "CTM_D_INPUT = 128\n",
    "CTM_ITERATIONS = 10\n",
    "CTM_HEADS = 4\n",
    "CTM_SYNCH_OUT = 128\n",
    "CTM_SYNCH_ACTION = 64\n",
    "CTM_SYNAPSE_DEPTH = 2\n",
    "CTM_MEMORY_LENGTH = 10\n",
    "CTM_MEMORY_HIDDEN = 32\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "LSTM_NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e04e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_ag_news(root='./data'):\n",
    "    \"\"\"Donwnloading AG_NEWS\"\"\"\n",
    "    url = \"https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz\"\n",
    "    data_path = os.path.join(root, 'ag_news_csv')\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        print(\"Dataset already downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"Downloading AG_NEWS dataset...\")\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        tgz_path = os.path.join(root, 'ag_news_csv.tgz')\n",
    "        urllib.request.urlretrieve(url, tgz_path)\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(tgz_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=root)\n",
    "        os.remove(tgz_path)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    train_data, test_data = [], []\n",
    "    with open(os.path.join(data_path, 'train.csv'), 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            # Класс, Заголовок, Описание\n",
    "            train_data.append((int(row[0]), row[1] + \" \" + row[2]))\n",
    "            \n",
    "    with open(os.path.join(data_path, 'test.csv'), 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            test_data.append((int(row[0]), row[1] + \" \" + row[2]))\n",
    "            \n",
    "    return train_data, test_data\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"Simple example of tokenization\"\"\"\n",
    "    return text.lower().strip().split()\n",
    "    # return word_tokenize(text.lower().strip())\n",
    "\n",
    "def build_vocab(data, tokenizer, max_size):\n",
    "    \"\"\"word -> index\"\"\"\n",
    "    counter = Counter()\n",
    "    for _, text in data:\n",
    "        counter.update(tokenizer(text))\n",
    "\n",
    "    most_common_words = [word for word, _ in counter.most_common(max_size - 2)] # -2 for <pad> и <unk>\n",
    "    \n",
    "    word_to_idx = {'<pad>': 0, '<unk>': 1}\n",
    "    for i, word in enumerate(most_common_words):\n",
    "        word_to_idx[word] = i + 2\n",
    "        \n",
    "    return word_to_idx\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "897d7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = download_and_extract_ag_news()\n",
    "word_to_idx = build_vocab(train_data, simple_tokenizer, VOCAB_SIZE_LIMIT)\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "PAD_IDX = word_to_idx['<pad>']\n",
    "NUM_CLASS = 4\n",
    "\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80624701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    unk_idx = word_to_idx['<unk>']\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(int(_label) - 1)\n",
    "        tokens = simple_tokenizer(_text)\n",
    "        indices = [word_to_idx.get(token, unk_idx) for token in tokens]\n",
    "        processed_text = torch.tensor(indices, dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "    padded_texts = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    return torch.tensor(label_list, dtype=torch.int64), padded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a74a62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, model_type='lstm'):\n",
    "    model.train()\n",
    "    total_acc, total_loss, total_count = 0, 0, 0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Training {model_type}')\n",
    "    for idx, (label, text) in enumerate(progress_bar):\n",
    "        label, text = label.to(DEVICE), text.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        if model_type == 'ctm':\n",
    "            predictions, _, _ = model(text)\n",
    "            logits = predictions[:, :, -1]\n",
    "        else:\n",
    "            logits = model(text)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_acc += (logits.argmax(1) == label).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        total_count += label.size(0)\n",
    "        progress_bar.set_postfix({'loss': total_loss / total_count, 'acc': total_acc / total_count})\n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "\n",
    "def evaluate(model, dataloader, criterion, model_type='lstm'):\n",
    "    model.eval()\n",
    "    total_acc, total_loss, total_count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label, text = label.to(DEVICE), text.to(DEVICE)\n",
    "            if model_type == 'ctm':\n",
    "                predictions, _, _ = model(text)\n",
    "                logits = predictions[:, :, -1]\n",
    "            else:\n",
    "                logits = model(text)\n",
    "            loss = criterion(logits, label)\n",
    "            total_acc += (logits.argmax(1) == label).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count, total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f50766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsDataset(train_data)\n",
    "test_dataset = NewsDataset(test_data)\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57f50766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using neuron select type: random-pairing\n",
      "Synch representation size action: 64\n",
      "Synch representation size out: 128\n",
      "Initializing CTM for NLP tasks...\n",
      "CTM_NLP initialized with vocab_size=10000, max_seq_len=512\n",
      "Output projection layer will map to 10000 logits.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "ctm_model = CTM_NLP(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    iterations=CTM_ITERATIONS,\n",
    "    d_model=CTM_D_MODEL,\n",
    "    d_input=CTM_D_INPUT,\n",
    "    # out_dims=NUM_CLASS,\n",
    "    heads=CTM_HEADS,\n",
    "    n_synch_out=CTM_SYNCH_OUT,\n",
    "    n_synch_action=CTM_SYNCH_ACTION,\n",
    "    synapse_depth=CTM_SYNAPSE_DEPTH,\n",
    "    memory_length=CTM_MEMORY_LENGTH,\n",
    "    deep_nlms=True,\n",
    "    memory_hidden_dims=CTM_MEMORY_HIDDEN,\n",
    "    do_layernorm_nlm=False,\n",
    "    dropout=0.2\n",
    ").to(DEVICE)\n",
    "    \n",
    "optimizer_ctm = torch.optim.AdamW(ctm_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "823e8f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ctm: 100%|██████████| 1875/1875 [14:56<00:00,  2.09it/s, loss=0.00931, acc=0.592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTM Epoch: 1, Time: 908.39s\n",
      "\tTrain Loss: 0.0093 | Train Acc: 59.24%\n",
      "\tTest Loss:  0.0039 | Test Acc:  68.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ctm: 100%|██████████| 1875/1875 [04:10<00:00,  7.50it/s, loss=0.00286, acc=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTM Epoch: 2, Time: 253.98s\n",
      "\tTrain Loss: 0.0029 | Train Acc: 70.15%\n",
      "\tTest Loss:  0.0032 | Test Acc:  69.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ctm: 100%|██████████| 1875/1875 [04:07<00:00,  7.57it/s, loss=0.00212, acc=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTM Epoch: 3, Time: 255.17s\n",
      "\tTrain Loss: 0.0021 | Train Acc: 71.42%\n",
      "\tTest Loss:  0.0032 | Test Acc:  69.88%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc, train_loss = train_epoch(ctm_model, train_dataloader, optimizer_ctm, criterion, model_type='ctm')\n",
    "        test_acc, test_loss = evaluate(ctm_model, test_dataloader, criterion, model_type='ctm')\n",
    "        \n",
    "        print(f'CTM Epoch: {epoch}, Time: {time.time() - epoch_start_time:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tTest Loss:  {test_loss:.4f} | Test Acc:  {test_acc*100:.2f}%')\n",
    "    \n",
    "results['CTM_NLP'] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d0d7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "           FINAL RESULTS\n",
      "========================================\n",
      "  CTM_NLP Test Accuracy:  69.88%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"           FINAL RESULTS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"  CTM_NLP Test Accuracy:  {results.get('CTM_NLP', 0)*100:.2f}%\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
