{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1225b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7789ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ctm_nlp import CTM_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff797a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 3,\n",
    "    \"d_model\": 512,        # Core CTM latent space\n",
    "    \"d_input\": 512,        # Embedding dimension\n",
    "    \"heads\": 8,\n",
    "    \"iterations\": 8,       # Number of \"thought\" steps\n",
    "    \"synapse_layers\": 4,   # Depth of the new Transformer synapse\n",
    "    \"memory_length\": 8,\n",
    "    \"n_synch\": 128,        # Number of neurons for sync representation\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "print(f\"Using device: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516c662",
   "metadata": {},
   "source": [
    "# 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e04e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ag_news_data(\n",
    "    data_dir='./data',\n",
    "    vocab_size=25000, # A reasonable vocabulary size\n",
    "    max_seq_len=256   # Cap sequence length to avoid excessive memory use\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the AG_NEWS dataset using the Hugging Face `datasets` library,\n",
    "    builds a vocabulary, and prepares DataLoaders for training and testing.\n",
    "    This version avoids using torchtext completely.\n",
    "    \"\"\"\n",
    "    print(\"Loading AG_NEWS dataset using Hugging Face `datasets`...\")\n",
    "    \n",
    "    # 1. Load the dataset from the Hugging Face Hub\n",
    "    # This is very robust and caches the data locally.\n",
    "    dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "    # 2. Train a tokenizer\n",
    "    # We will train a simple WordLevel tokenizer on the training data.\n",
    "    # This is more flexible and modern than the old torchtext vocab system.\n",
    "    tokenizer_path = os.path.join(data_dir, 'ag_news_tokenizer.json')\n",
    "    \n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        print(\"Training a new tokenizer...\")\n",
    "        # Initialize a tokenizer\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "        # Create a trainer\n",
    "        trainer = WordLevelTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=[\"<unk>\", \"<pad>\", \"<cls>\", \"<sep>\"]\n",
    "        )\n",
    "\n",
    "        # A generator function to feed text to the trainer\n",
    "        def get_training_corpus():\n",
    "            for i in range(len(dataset[\"train\"])):\n",
    "                yield dataset[\"train\"][i][\"text\"]\n",
    "\n",
    "        # Train the tokenizer\n",
    "        tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "        \n",
    "        # Save the tokenizer for future use\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "        tokenizer.save(tokenizer_path)\n",
    "    else:\n",
    "        print(f\"Loading tokenizer from {tokenizer_path}\")\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    # Get the vocabulary size and padding token ID from the trained tokenizer\n",
    "    actual_vocab_size = tokenizer.get_vocab_size()\n",
    "    padding_idx = tokenizer.token_to_id(\"<pad>\")\n",
    "    \n",
    "    print(f\"Vocabulary size: {actual_vocab_size}\")\n",
    "    print(f\"Padding index: {padding_idx}\")\n",
    "\n",
    "    # 3. Create a preprocessing function\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts and truncate to max_seq_len\n",
    "        tokenized_inputs = tokenizer.encode_batch(examples[\"text\"])\n",
    "        \n",
    "        # Extract input_ids and create attention_masks\n",
    "        input_ids = [encoding.ids[:max_seq_len] for encoding in tokenized_inputs]\n",
    "        \n",
    "        # Labels in 'ag_news' from `datasets` are already 0-indexed (0-3)\n",
    "        return {\"input_ids\": input_ids, \"labels\": examples[\"label\"]}\n",
    "\n",
    "    print(\"Tokenizing and formatting the dataset...\")\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Set the format to PyTorch tensors\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "    # 4. Define the collate function\n",
    "    def collate_batch(batch):\n",
    "        # The batch is now a list of dictionaries\n",
    "        input_ids_list = [item['input_ids'] for item in batch]\n",
    "        labels_list = [item['labels'] for item in batch]\n",
    "        \n",
    "        # Pad sequences to the max length in this batch\n",
    "        padded_texts = nn.utils.rnn.pad_sequence(\n",
    "            input_ids_list, \n",
    "            batch_first=True, \n",
    "            padding_value=padding_idx\n",
    "        )\n",
    "        \n",
    "        # Create attention masks (1 for real tokens, 0 for padding)\n",
    "        attention_masks = (padded_texts != padding_idx).int()\n",
    "        \n",
    "        labels = torch.tensor(labels_list, dtype=torch.int64)\n",
    "        \n",
    "        return padded_texts, attention_masks, labels\n",
    "\n",
    "    # 5. Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_batch)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    return train_dataloader, test_dataloader, actual_vocab_size, padding_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab8ec7",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7448784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "    for padded_texts, attention_masks, labels in progress_bar:\n",
    "        padded_texts = padded_texts.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The model returns (predictions, certainties, final_sync_state)\n",
    "        predictions, _, _ = model(padded_texts, attention_mask=attention_masks)\n",
    "        \n",
    "        # For classification, we use the output from the FINAL thought step\n",
    "        logits = predictions[:, :, -1]\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += (logits.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': total_loss / total_count, 'acc': total_acc / total_count})\n",
    "\n",
    "    return total_loss / total_count, total_acc / total_count\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for padded_texts, attention_masks, labels in progress_bar:\n",
    "            padded_texts = padded_texts.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions, _, _ = model(padded_texts, attention_mask=attention_masks)\n",
    "            logits = predictions[:, :, -1]\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_acc += (logits.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': total_loss / total_count, 'acc': total_acc / total_count})\n",
    "\n",
    "    return total_loss / total_count, total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897d7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AG_NEWS dataset using Hugging Face `datasets`...\n",
      "Loading tokenizer from ./data\\ag_news_tokenizer.json\n",
      "Vocabulary size: 25000\n",
      "Padding index: 1\n",
      "Tokenizing and formatting the dataset...\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "# --- Data ---\n",
    "train_loader, test_loader, vocab_size, padding_idx = get_ag_news_data()\n",
    "num_classes = 4\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70813bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using neuron select type: random-pairing\n",
      "Synch representation size action: 128\n",
      "Synch representation size out: 128\n",
      "Initializing CTM for NLP...\n",
      "Replacing SynapseUnet with TransformerEncoder (4 layers)...\n",
      "Model configured for 4-class classification.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing a dummy forward pass to initialize lazy layers...\n",
      "Lazy layers initialized successfully.\n",
      "\n",
      "Model created with 27,321,101 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# --- Model ---\n",
    "model = CTM_NLP(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    padding_idx=padding_idx,\n",
    "    d_model=CONFIG['d_model'],\n",
    "    d_input=CONFIG['d_input'],\n",
    "    heads=CONFIG['heads'],\n",
    "    iterations=CONFIG['iterations'],\n",
    "    synapse_depth=CONFIG['synapse_layers'],\n",
    "    memory_length=CONFIG['memory_length'],\n",
    "    n_synch_out=CONFIG['n_synch'],\n",
    "    n_synch_action=CONFIG['n_synch'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    deep_nlms=True, \n",
    "    do_layernorm_nlm=True,\n",
    ").to(CONFIG['device'])\n",
    "    \n",
    "print(\"\\nPerforming a dummy forward pass to initialize lazy layers...\")\n",
    "try:\n",
    "        # Create a small dummy batch on the correct device\n",
    "        dummy_batch_size = 2\n",
    "        dummy_seq_len = 16\n",
    "        dummy_input_ids = torch.randint(\n",
    "            0, vocab_size, \n",
    "            (dummy_batch_size, dummy_seq_len), \n",
    "            device=CONFIG['device']\n",
    "        )\n",
    "        dummy_attention_mask = torch.ones_like(dummy_input_ids)\n",
    "\n",
    "        # Run the dummy forward pass\n",
    "        with torch.no_grad():\n",
    "            model(dummy_input_ids, attention_mask=dummy_attention_mask)\n",
    "        \n",
    "        print(\"Lazy layers initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "        print(f\"Error during dummy forward pass: {e}\")\n",
    "        print(\"Please check model architecture and input dimensions.\")\n",
    "        # Exit or raise the error if initialization fails\n",
    "        raise e\n",
    "\n",
    "# Now it is safe to count parameters and create the optimizer\n",
    "print(f\"\\nModel created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
    "\n",
    "# --- Optimizer and Loss ---\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a62ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [15:04<00:00,  4.15it/s, loss=0.0238, acc=0.673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.0238 | Train Acc: 0.6731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 238/238 [00:16<00:00, 14.68it/s, loss=0.0149, acc=0.836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Eval Loss: 0.0149  | Eval Acc: 0.8358\n",
      "\n",
      "--- Epoch 2/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 121/3750 [00:45<16:02,  3.77it/s, loss=0.0147, acc=0.831]"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    print(f\"\\n--- Epoch {epoch}/{CONFIG['epochs']} ---\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, CONFIG['device'])\n",
    "    print(f\"Epoch {epoch} Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "    eval_loss, eval_acc = evaluate(model, test_loader, criterion, CONFIG['device'])\n",
    "    print(f\"Epoch {epoch} Eval Loss: {eval_loss:.4f}  | Eval Acc: {eval_acc:.4f}\")\n",
    "\n",
    "print(\"\\n--- Final Test Evaluation ---\")\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, CONFIG['device'])\n",
    "print(f\"Final Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
